import numpy as np
from mytorch import tensor
from mytorch.tensor import Tensor
from mytorch.nn.module import Module
from mytorch.nn.activations import Tanh, ReLU, Sigmoid
from mytorch.nn.util import PackedSequence, pack_sequence, unpack_sequence


class RNNUnit(Module):
    '''
    This class defines a single RNN Unit block.

    Args:
        input_size (int): # features in each timestep
        hidden_size (int): # features generated by the RNNUnit at each timestep
        nonlinearity (string): Non-linearity to be applied the result of matrix operations 
    '''

    def __init__(self, input_size, hidden_size, nonlinearity = 'tanh' ):
        
        super(RNNUnit,self).__init__()
        
        # Initializing parameters
        self.weight_ih = Tensor(np.random.randn(hidden_size,input_size), requires_grad=True, is_parameter=True)
        self.bias_ih   = Tensor(np.zeros(hidden_size), requires_grad=True, is_parameter=True)
        self.weight_hh = Tensor(np.random.randn(hidden_size,hidden_size), requires_grad=True, is_parameter=True)
        self.bias_hh   = Tensor(np.zeros(hidden_size), requires_grad=True, is_parameter=True)

        self.hidden_size = hidden_size
        
        # Setting the Activation Unit
        if nonlinearity == 'tanh':
            self.act = Tanh()
        elif nonlinearity == 'relu':
            self.act = ReLU()

    def __call__(self, input, hidden = None):
        return self.forward(input,hidden)

    def forward(self, input, hidden = None):
        '''
        Args:
            input (Tensor): (effective_batch_size,input_size)
            hidden (Tensor,None): (effective_batch_size,hidden_size)
        Return:
            Tensor: (effective_batch_size,hidden_size)
        '''
        
        # TODO: INSTRUCTIONS
        # Perform matrix operations to construct the intermediary value from input and hidden tensors
        # Apply the activation on the resultant
        # Remeber to handle the case when hidden = None. Construct a tensor of appropriate size, filled with 0s to use as the hidden.
        batch_size = input.shape[0]
        if not hidden:
            hidden = Tensor(np.zeros((batch_size, self.hidden_size)))

        z = input @ self.weight_ih.T() + self.bias_ih + \
            hidden @ self.weight_hh.T() + self.bias_hh
        h_prime = self.act(z)

        return h_prime

        
        # raise NotImplementedError('Implement Forward')


class TimeIterator(Module):
    '''
    For a given input this class iterates through time by processing the entire
    seqeunce of timesteps. Can be thought to represent a single layer for a 
    given basic unit which is applied at each time step.
    
    Args:
        basic_unit (Class): RNNUnit or GRUUnit. This class is used to instantiate the unit that will be used to process the inputs
        input_size (int): # features in each timestep
        hidden_size (int): # features generated by the RNNUnit at each timestep
        nonlinearity (string): Non-linearity to be applied the result of matrix operations 

    '''

    def __init__(self, basic_unit, input_size, hidden_size, nonlinearity = 'tanh' ):
        super(TimeIterator,self).__init__()

        # basic_unit can either be RNNUnit or GRUUnit
        self.unit = basic_unit(input_size,hidden_size,nonlinearity)    

    def __call__(self, input, hidden = None):
        return self.forward(input,hidden)
    
    def forward(self,input,hidden = None):
        
        '''
        NOTE: Please get a good grasp on util.PackedSequence before attempting this.

        Args:
            input (PackedSequence): input.data is tensor of shape ( total number of timesteps (sum) across all samples in the batch, input_size)
            hidden (Tensor, None): (batch_size, hidden_size)
        Returns:
            PackedSequence: ( total number of timesteps (sum) across all samples in the batch, hidden_size)
            Tensor (batch_size,hidden_size): This is the hidden generated by the last time step for each sample joined together.
        '''
        
        # Resolve the PackedSequence into its components
        data, sorted_indices, batch_sizes = input

        last_hiddens = [] # hidden generated by the last time step for each sample joined together
        outputs = []

        features = data[:batch_sizes[0]]
        hidden = self.unit(input = features, hidden = hidden)
        outputs.append(hidden)
        start_idx = batch_sizes[0]


        for t in range(1, len(batch_sizes)): # iterate along the time steps
            # input @ t
            features = data[start_idx:start_idx + batch_sizes[t]]

            diff = hidden.shape[0] - features.shape[0]
            if diff > 0: # when batch size@t < batch size@t-1
                # print("t = ", t+1, "diff = ", diff, "hidden {} = \n".format(t), hidden, "\n last:", hidden[0-diff:])
                last_hiddens.append(hidden[0 - diff:])

            # hidden state @ t
            hidden = self.unit(input = features, hidden = hidden[:batch_sizes[t]])
            outputs.append(hidden)
            #update idx
            start_idx = start_idx + batch_sizes[t]

        #append last hiddens
        last_hiddens.append(hidden[0-batch_sizes[-1]:])

        hidden_tensor = tensor.cat(last_hiddens, 0)
        output_tensor = tensor.cat(outputs, 0)

        assert hidden_tensor.shape[0] == len(sorted_indices)
        assert output_tensor.shape[0] == data.shape[0]
        # orig_order = np.argsort(sorted_indices)
        # hidden_tensor = hidden_tensor[orig_order]
        # output_tensor = output_tensor[orig_order]

        return PackedSequence(data = output_tensor, sorted_indices=sorted_indices, batch_sizes = batch_sizes), hidden_tensor[::-1]



        
        # TODO: INSTRUCTIONS
        # Iterate over appropriate segments of the "data" tensor to pass same timesteps across all samples in the batch simultaneously to the unit for processing.
        # Remeber to account for scenarios when effective_batch_size changes between one iteration to the next

        # raise NotImplementedError('Implement Forward')


class RNN(TimeIterator):
    '''
    Child class for TimeIterator which appropriately initializes the parent class to construct an RNN.
    Args:
        input_size (int): # features in each timestep
        hidden_size (int): # features generated by the RNNUnit at each timestep
        nonlinearity (string): Non-linearity to be applied the result of matrix operations 
    '''

    def __init__(self, input_size, hidden_size, nonlinearity = 'tanh' ):
        # TODO: Properly Initialize the RNN class
        # raise NotImplementedError('Initialize properly!'))
        super().__init__(basic_unit = RNNUnit,
                         input_size=input_size,
                         hidden_size=hidden_size,
                         nonlinearity=nonlinearity)

