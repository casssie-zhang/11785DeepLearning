{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":3710,"status":"ok","timestamp":1608373628391,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"vN6qjxTP1Tim"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch.nn import BatchNorm1d, ReLU, CrossEntropyLoss\n","from torch import optim\n","from torch.optim.lr_scheduler import StepLR, MultiplicativeLR, CosineAnnealingLR\n","\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"OIFW2ZiU1Tiq"},"source":["## 1 Load Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56261,"status":"ok","timestamp":1608373943109,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"B_rI0pEo1cEW","outputId":"1dfdf14d-7313-4711-cbc7-4dd18579cf68"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3796,"status":"ok","timestamp":1608374142906,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"xMUyTzEzkF2q","outputId":"8614664e-a2f9-4975-f498-81ac1dc33860"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n","Requirement already up-to-date: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.10)\n","Requirement already satisfied, skipping upgrade: six\u003e=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n","Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n","Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n","Requirement already satisfied, skipping upgrade: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n","Requirement already satisfied, skipping upgrade: text-unidecode\u003e=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify-\u003ekaggle) (1.3)\n","Requirement already satisfied, skipping upgrade: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-\u003ekaggle) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-\u003ekaggle) (2.10)\n"]}],"source":["%cd /content\n","!pip install --upgrade kaggle\n","!mkdir .kaggle"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1114,"status":"ok","timestamp":1608374144488,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"vsn-2Pa9kW-m"},"outputs":[],"source":["import json\n","token = {\"username\":\"kexinzhangcmu\",\"key\":\"91f327ebd62f2d3cfeb09f0344748606\"}\n","\n","with open('/content/.kaggle/kaggle.json', 'w') as file:\n","      json.dump(token, file)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41468,"status":"ok","timestamp":1608374607146,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"-q3kA4MmkLHI"},"outputs":[{"name":"stdout","output_type":"stream","text":["- path is now set to: /content/\n","Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n","Downloading sample.csv.zip to /content/competitions/11-785-fall-20-homework-1-part-2\n","  0% 0.00/3.36M [00:00\u003c?, ?B/s]\n","100% 3.36M/3.36M [00:00\u003c00:00, 55.9MB/s]\n","Downloading train_labels.npy.zip to /content/competitions/11-785-fall-20-homework-1-part-2\n"," 53% 5.00M/9.45M [00:00\u003c00:00, 22.0MB/s]\n","100% 9.45M/9.45M [00:00\u003c00:00, 31.5MB/s]\n","Downloading test.npy.zip to /content/competitions/11-785-fall-20-homework-1-part-2\n"," 99% 97.0M/98.1M [00:03\u003c00:00, 22.5MB/s]\n","100% 98.1M/98.1M [00:03\u003c00:00, 33.0MB/s]\n","Downloading phones.txt to /content/competitions/11-785-fall-20-homework-1-part-2\n","  0% 0.00/3.19k [00:00\u003c?, ?B/s]\n","100% 3.19k/3.19k [00:00\u003c00:00, 3.05MB/s]\n","Downloading dev_labels.npy.zip to /content/competitions/11-785-fall-20-homework-1-part-2\n","  0% 0.00/622k [00:00\u003c?, ?B/s]\n","100% 622k/622k [00:00\u003c00:00, 194MB/s]\n","Downloading train.npy.zip to /content/competitions/11-785-fall-20-homework-1-part-2\n"," 99% 1.56G/1.57G [00:32\u003c00:00, 38.8MB/s]\n","100% 1.57G/1.57G [00:32\u003c00:00, 52.5MB/s]\n","Downloading dev.npy.zip to /content/competitions/11-785-fall-20-homework-1-part-2\n"," 82% 81.0M/98.6M [00:02\u003c00:00, 22.5MB/s]\n","100% 98.6M/98.6M [00:02\u003c00:00, 38.0MB/s]\n"]}],"source":["!chmod 600 /content/.kaggle/kaggle.json\n","!mkdir ~/.kaggle\n","!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n","!kaggle config set -n path -v /content/\n","!kaggle competitions download -c 11-785-fall-20-homework-1-part-2"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50664,"status":"ok","timestamp":1608374745270,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"uYSDZe4XmWcR","outputId":"9c4ac7db-e13d-4cef-db7c-4b26602929e4"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/competitions/11-785-fall-20-homework-1-part-2\n","Archive:  train.npy.zip\n","  inflating: train.npy               \n","\n","Archive:  dev_labels.npy.zip\n","  inflating: dev_labels.npy          \n","\n","Archive:  dev.npy.zip\n","  inflating: dev.npy                 \n","\n","Archive:  test.npy.zip\n","  inflating: test.npy                \n","\n","Archive:  train_labels.npy.zip\n","  inflating: train_labels.npy        \n","\n","Archive:  sample.csv.zip\n","  inflating: sample.csv              \n","\n","6 archives were successfully processed.\n"]}],"source":["%cd /content/competitions/11-785-fall-20-homework-1-part-2\n","!unzip \\*.zip\n","!rm -rf *.zip"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1001,"status":"ok","timestamp":1608373994361,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"Hufi9Joe1Tiq"},"outputs":[],"source":["context_size = 15"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":26868,"status":"ok","timestamp":1608374797651,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"dt5jBywV1Tiu"},"outputs":[],"source":["DATA_PATH=\"/content/competitions/11-785-fall-20-homework-1-part-2/\"\n","def load_data(DATA_PATH):\n","    \"\"\"load data\"\"\"\n","    train = np.load(DATA_PATH + \"train.npy\",allow_pickle=True)\n","    train_labels = np.load(DATA_PATH + \"train_labels.npy\",allow_pickle=True)\n","    dev = np.load(DATA_PATH + 'dev.npy', allow_pickle=True)\n","    dev_labels = np.load(DATA_PATH + 'dev_labels.npy', allow_pickle=True)\n","    \n","    return train, train_labels, dev, dev_labels \n","\n","\n","train, train_labels, dev, dev_labels = load_data(DATA_PATH)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1220,"status":"ok","timestamp":1608373970692,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"h0XeT0Xw3Kzl","outputId":"714752de-48c7-42f5-b8db-9a83a52ecc11"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4155,"status":"ok","timestamp":1608374833604,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"NbRXt5iz1Tiw","outputId":"538da8ec-727f-4cbf-c526-cd651ac4d903"},"outputs":[{"name":"stdout","output_type":"stream","text":["# of examples: 27329567 // 27329537\n"]}],"source":["class hw1Dataset(Dataset):\n","    def __init__(self, X, Y, context_size = 12):\n","        self.context_size = context_size\n","        self.span = 2 * self.context_size + 1\n","\n","        features = np.pad(np.concatenate(X), \n","                          pad_width = ((context_size, context_size), (0,0)))\n","        \n","        labels = np.concatenate(Y)\n","        \n","        # assert(len(features) == len(labels))\n","\n","        print(\"# of examples:\", len(features), \"//\", len(labels))\n","        self.X = torch.tensor(features)\n","        self.Y = torch.tensor(labels)\n","    \n","    def __len__(self):\n","        return len(self.Y)\n","    \n","    def __getitem__(self, index):\n","        X = self.X[index : index + self.span].float().reshape(-1)\n","        Y = self.Y[index].long().reshape(-1)\n","                \n","        return X, Y\n","    \n","\n","num_workers = 4 if cuda else 0\n","\n","train_dateset = hw1Dataset(train, train_labels, context_size)\n","train_loader_args = dict(shuffle = True, batch_size = 512,\n","                        num_workers = num_workers, pin_memory = True, drop_last = True)\\\n","if cuda else dict(shuffle = True, batch_size = 512)\n","\n","train_loader = DataLoader(train_dateset, **train_loader_args)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1071,"status":"ok","timestamp":1608374915612,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"AHmfTaES1Tiy","outputId":"136e6420-efc0-46f9-8447-35e3cbc1c487"},"outputs":[{"name":"stdout","output_type":"stream","text":["# of examples: 1598434 // 1598404\n"]}],"source":["dev_dataset = hw1Dataset(dev, dev_labels, context_size)\n","dev_loader_args = dict(shuffle = False, batch_size = 512,\n","                        num_workers = num_workers, pin_memory = True, drop_last = True)\\\n","if cuda else dict(shuffle = False, batch_szie = 512)\n","\n","dev_loader = DataLoader(dev_dataset, **dev_loader_args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBMI8BGw63xR"},"outputs":[],"source":["del train\n","del train_labels\n","del dev\n","del dev_labels"]},{"cell_type":"markdown","metadata":{"id":"tytlsCYr1Ti1"},"source":["## 2 Model Architechture"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":1133,"status":"ok","timestamp":1608375013910,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"uTZdC_JB1Ti2"},"outputs":[],"source":["# create a more customizable network module (equivalent here)\n","class MyNetwork(torch.nn.Module):\n","    # you can use the layer sizes as initialization arguments if you want to\n","    def __init__(self,input_size, output_size=346):\n","        super().__init__()\n","        \n","\n","        layers = [\n","                  \n","                  torch.nn.Linear(input_size,2048),\n","                  ReLU(),\n","                  BatchNorm1d(2048),\n","\n","                  torch.nn.Linear(2048, 2048),\n","                  ReLU(),\n","                  BatchNorm1d(2048),\n","\n","                  torch.nn.Linear(2048, 1024),\n","                  ReLU(),\n","                  BatchNorm1d(1024),\n","\n","                  torch.nn.Linear(1024, 1024),\n","                  ReLU(),\n","                  BatchNorm1d(1024),\n","\n","                  torch.nn.Linear(1024, 1024),\n","                  ReLU(),\n","                  BatchNorm1d(1024),\n","\n","                  torch.nn.Linear(1024, 1024),\n","                  ReLU(),\n","                  BatchNorm1d(1024),\n","\n","                  torch.nn.Linear(1024, 1024),\n","                  ReLU(),\n","                  BatchNorm1d(1024),\n","\n","                  torch.nn.Linear(1024, 1024),\n","                  ReLU(),\n","                  BatchNorm1d(1024),\n","\n","                  torch.nn.Linear(1024, 768),\n","                  ReLU(),\n","                  BatchNorm1d(768),\n","\n","                  torch.nn.Linear(768, 512),\n","                  ReLU(),\n","                  BatchNorm1d(512),\n","\n","                  torch.nn.Linear(512, 346)\n","        ]\n","\n","        self.layers = torch.nn.Sequential(*layers)\n","\n","    def forward(self, input_val):\n","        return self.layers(input_val)\n","\n","model = MyNetwork(13 * (2 * context_size + 1),346)"]},{"cell_type":"markdown","metadata":{"id":"5SkfVakt1Ti6"},"source":["## 3 Set Hyperparameters"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":1055,"status":"ok","timestamp":1608376085527,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"QcE5Kzdb1Ti7","scrolled":true},"outputs":[],"source":["device = torch.device(\"cuda\" if cuda else \"GPU\")\n","model.to(device)\n","NUM_EPOCHS = 4 # TODO: for test train 1 epoch only\n","criterion = CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr = 1e-3, weight_decay = 5e-5)\n","\n","lmbda = lambda epoch: 0.85\n","scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)"]},{"cell_type":"markdown","metadata":{"id":"iFedDIBa1Ti-"},"source":["## 4 Train model"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":945,"status":"ok","timestamp":1608375057117,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"bIY0HZG0CckO"},"outputs":[],"source":["def validate(model, dataloader):\n","    \"\"\"\n","    Validation routine, tests on val data, scores accuracy\n","    Relevant Args:\n","        dev dataset loader\n","    Returns:\n","        float: Accuracy = correct / total\n","    \"\"\"\n","    #TODO: implement validation based on pseudocode\n","    model.eval()\n","    \n","    total = 0\n","    num_correct = 0\n","    with torch.no_grad():\n","      for idx, (data, labels) in enumerate(dataloader):\n","        data = data.to(device)\n","        labels = labels.to(device)\n","              \n","        out = model.forward(data)\n","        _, predicted = torch.max(out.data, 1)\n","        num_correct += (predicted == labels.squeeze()).sum().item()\n","        total += labels.size(0)\n","            \n","    return num_correct / total"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":971,"status":"ok","timestamp":1608395307068,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"_QWb_mjv1Ti-"},"outputs":[],"source":["def train_for_one_epoch(model, train_loader, dev_loader, optimizer, model_name:str, epoch:int, scheduler = None, if_save = False):\n","    \"\"\"\n","    train for one epoch\n","    \"\"\"\n","    model.train()\n","    before = time.time()\n","    \n","    for idx, (x, y) in enumerate(train_loader):\n","\n","        model.train()\n","\n","        optimizer.zero_grad()\n","        \n","        x = x.to(device)\n","        y = y.to(device)\n","        \n","        output = model(x)\n","\n","\n","        loss = criterion(output, y.squeeze()) # softmax\n","        \n","        loss.backward()\n","        optimizer.step() # update the weights using the computed gradients\n","        \n","\n","        if idx % 2000 == 1999:\n","            model.eval()\n","            # dev_accuracy = validate(model, dev_loader)\n","            \n","            print(\"Epoch {}/{}\\tTraining Loss: {:.3f}\\ttakes {:.3f} seconds\"\\\n","                .format(epoch+1, idx + 1, loss.item(), int(time.time()-before)))\n","            before=time.time()\n","            model.train()\n","\n","\n","            \n","    if scheduler:    \n","        scheduler.step()\n","    \n","    model.eval()\n","    dev_accuracy = validate(model, dev_loader)\n","    print(\"Epoch {}/{}\\tTraining Loss: {:.3f}\\tDev Accuracy: {:.3f}\\ttakes {:.3f} seconds\"\\\n","          .format(epoch+1, idx + 1, loss.item(), dev_accuracy, int(time.time()-before)))\n","    before=time.time()\n","\n","    if if_save:\n","\n","        torch.save({\n","                        'model_state_dict': model.state_dict(),\n","                        'optimizer_state_dict': optimizer.state_dict(),\n","            }, \"/content/gdrive/MyDrive/11685deeplearning/hw1p2\" + model_name+\"_\"+\"{:.3f}\".format(dev_accuracy))"]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1141,"status":"ok","timestamp":1608394791101,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"Wom5livPYAnV","outputId":"fbfe8847-94c3-45cd-b77a-319447d963cf"},"outputs":[{"data":{"text/plain":["[5.2200625000000005e-05]"]},"execution_count":49,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["scheduler.get_last_lr()"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":4404,"status":"ok","timestamp":1608395354362,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"VAlOCwZz1PIO"},"outputs":[],"source":["cp /content/competitions/11-785-fall-20-homework-1-part-2/model* /content/gdrive/MyDrive/11685deeplearning/hw1p2"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4074703,"status":"ok","timestamp":1608401535387,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"1DyHAt0E4ERR","outputId":"2cbf9ce5-171a-44f7-e67b-f3784a6441f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/2000\tTraining Loss: 0.928\ttakes 35.000 seconds\n","Epoch 1/4000\tTraining Loss: 0.904\ttakes 29.000 seconds\n","Epoch 1/6000\tTraining Loss: 0.982\ttakes 29.000 seconds\n","Epoch 1/8000\tTraining Loss: 0.903\ttakes 30.000 seconds\n","Epoch 1/10000\tTraining Loss: 1.012\ttakes 29.000 seconds\n","Epoch 1/12000\tTraining Loss: 1.062\ttakes 29.000 seconds\n","Epoch 1/14000\tTraining Loss: 0.849\ttakes 29.000 seconds\n","Epoch 1/16000\tTraining Loss: 0.802\ttakes 29.000 seconds\n","Epoch 1/18000\tTraining Loss: 0.829\ttakes 29.000 seconds\n","Epoch 1/20000\tTraining Loss: 0.982\ttakes 29.000 seconds\n","Epoch 1/22000\tTraining Loss: 0.958\ttakes 28.000 seconds\n","Epoch 1/24000\tTraining Loss: 0.811\ttakes 28.000 seconds\n","Epoch 1/26000\tTraining Loss: 0.802\ttakes 28.000 seconds\n","Epoch 1/28000\tTraining Loss: 0.881\ttakes 28.000 seconds\n","Epoch 1/30000\tTraining Loss: 0.900\ttakes 28.000 seconds\n","Epoch 1/32000\tTraining Loss: 0.946\ttakes 29.000 seconds\n","Epoch 1/34000\tTraining Loss: 1.017\ttakes 28.000 seconds\n","Epoch 1/36000\tTraining Loss: 0.999\ttakes 29.000 seconds\n","Epoch 1/38000\tTraining Loss: 0.897\ttakes 29.000 seconds\n","Epoch 1/40000\tTraining Loss: 0.822\ttakes 29.000 seconds\n","Epoch 1/42000\tTraining Loss: 0.814\ttakes 29.000 seconds\n","Epoch 1/44000\tTraining Loss: 0.888\ttakes 28.000 seconds\n","Epoch 1/46000\tTraining Loss: 0.951\ttakes 28.000 seconds\n","Epoch 1/48000\tTraining Loss: 1.013\ttakes 28.000 seconds\n","Epoch 1/50000\tTraining Loss: 1.000\ttakes 28.000 seconds\n","Epoch 1/52000\tTraining Loss: 0.844\ttakes 28.000 seconds\n","Epoch 1/54000\tTraining Loss: 1.029\ttakes 28.000 seconds\n","Epoch 1/56000\tTraining Loss: 0.997\ttakes 28.000 seconds\n","Epoch 1/58000\tTraining Loss: 0.911\ttakes 29.000 seconds\n","Epoch 1/60000\tTraining Loss: 0.968\ttakes 29.000 seconds\n","Epoch 1/62000\tTraining Loss: 1.021\ttakes 28.000 seconds\n","Epoch 1/64000\tTraining Loss: 0.907\ttakes 28.000 seconds\n","Epoch 1/66000\tTraining Loss: 0.926\ttakes 28.000 seconds\n","Epoch 1/68000\tTraining Loss: 0.911\ttakes 29.000 seconds\n","Epoch 1/70000\tTraining Loss: 0.896\ttakes 28.000 seconds\n","Epoch 1/72000\tTraining Loss: 0.785\ttakes 28.000 seconds\n","Epoch 1/74000\tTraining Loss: 1.059\ttakes 27.000 seconds\n","Epoch 1/76000\tTraining Loss: 1.015\ttakes 28.000 seconds\n","Epoch 1/78000\tTraining Loss: 0.922\ttakes 28.000 seconds\n","Epoch 1/80000\tTraining Loss: 0.811\ttakes 28.000 seconds\n","Epoch 1/82000\tTraining Loss: 0.802\ttakes 28.000 seconds\n","Epoch 1/84000\tTraining Loss: 0.896\ttakes 28.000 seconds\n","Epoch 1/86000\tTraining Loss: 0.933\ttakes 28.000 seconds\n","Epoch 1/88000\tTraining Loss: 0.979\ttakes 28.000 seconds\n","Epoch 1/90000\tTraining Loss: 1.012\ttakes 27.000 seconds\n","Epoch 1/92000\tTraining Loss: 0.874\ttakes 27.000 seconds\n","Epoch 1/94000\tTraining Loss: 0.898\ttakes 28.000 seconds\n","Epoch 1/96000\tTraining Loss: 0.965\ttakes 27.000 seconds\n","Epoch 1/98000\tTraining Loss: 0.918\ttakes 28.000 seconds\n","Epoch 1/100000\tTraining Loss: 0.906\ttakes 28.000 seconds\n","Epoch 1/102000\tTraining Loss: 1.112\ttakes 29.000 seconds\n","Epoch 1/104000\tTraining Loss: 0.934\ttakes 27.000 seconds\n","Epoch 1/106000\tTraining Loss: 0.856\ttakes 28.000 seconds\n","Epoch 1/106756\tTraining Loss: 0.797\tDev Accuracy: 0.698\ttakes 31.000 seconds\n","Epoch 2/2000\tTraining Loss: 0.886\ttakes 31.000 seconds\n","Epoch 2/4000\tTraining Loss: 1.054\ttakes 28.000 seconds\n","Epoch 2/6000\tTraining Loss: 0.885\ttakes 27.000 seconds\n","Epoch 2/8000\tTraining Loss: 0.865\ttakes 28.000 seconds\n","Epoch 2/10000\tTraining Loss: 0.808\ttakes 28.000 seconds\n","Epoch 2/12000\tTraining Loss: 0.810\ttakes 27.000 seconds\n","Epoch 2/14000\tTraining Loss: 0.977\ttakes 28.000 seconds\n","Epoch 2/16000\tTraining Loss: 0.889\ttakes 29.000 seconds\n","Epoch 2/18000\tTraining Loss: 0.931\ttakes 27.000 seconds\n","Epoch 2/20000\tTraining Loss: 0.955\ttakes 27.000 seconds\n","Epoch 2/22000\tTraining Loss: 0.966\ttakes 28.000 seconds\n","Epoch 2/24000\tTraining Loss: 0.864\ttakes 28.000 seconds\n","Epoch 2/26000\tTraining Loss: 0.941\ttakes 27.000 seconds\n","Epoch 2/28000\tTraining Loss: 0.798\ttakes 28.000 seconds\n","Epoch 2/30000\tTraining Loss: 0.885\ttakes 28.000 seconds\n","Epoch 2/32000\tTraining Loss: 1.060\ttakes 28.000 seconds\n","Epoch 2/34000\tTraining Loss: 0.904\ttakes 27.000 seconds\n","Epoch 2/36000\tTraining Loss: 0.975\ttakes 27.000 seconds\n","Epoch 2/38000\tTraining Loss: 0.668\ttakes 29.000 seconds\n","Epoch 2/40000\tTraining Loss: 0.943\ttakes 29.000 seconds\n","Epoch 2/42000\tTraining Loss: 0.913\ttakes 27.000 seconds\n","Epoch 2/44000\tTraining Loss: 0.959\ttakes 28.000 seconds\n","Epoch 2/46000\tTraining Loss: 1.134\ttakes 27.000 seconds\n","Epoch 2/48000\tTraining Loss: 0.770\ttakes 28.000 seconds\n","Epoch 2/50000\tTraining Loss: 0.935\ttakes 27.000 seconds\n","Epoch 2/52000\tTraining Loss: 0.851\ttakes 28.000 seconds\n","Epoch 2/54000\tTraining Loss: 0.869\ttakes 27.000 seconds\n","Epoch 2/56000\tTraining Loss: 0.989\ttakes 28.000 seconds\n","Epoch 2/58000\tTraining Loss: 0.865\ttakes 28.000 seconds\n","Epoch 2/60000\tTraining Loss: 0.789\ttakes 28.000 seconds\n","Epoch 2/62000\tTraining Loss: 0.874\ttakes 28.000 seconds\n","Epoch 2/64000\tTraining Loss: 0.919\ttakes 28.000 seconds\n","Epoch 2/66000\tTraining Loss: 1.027\ttakes 28.000 seconds\n","Epoch 2/68000\tTraining Loss: 0.909\ttakes 28.000 seconds\n","Epoch 2/70000\tTraining Loss: 0.843\ttakes 28.000 seconds\n","Epoch 2/72000\tTraining Loss: 1.000\ttakes 28.000 seconds\n","Epoch 2/74000\tTraining Loss: 0.855\ttakes 27.000 seconds\n","Epoch 2/76000\tTraining Loss: 1.168\ttakes 27.000 seconds\n","Epoch 2/78000\tTraining Loss: 0.904\ttakes 28.000 seconds\n","Epoch 2/80000\tTraining Loss: 0.977\ttakes 27.000 seconds\n","Epoch 2/82000\tTraining Loss: 0.936\ttakes 28.000 seconds\n","Epoch 2/84000\tTraining Loss: 0.948\ttakes 28.000 seconds\n","Epoch 2/86000\tTraining Loss: 0.947\ttakes 28.000 seconds\n","Epoch 2/88000\tTraining Loss: 1.005\ttakes 28.000 seconds\n","Epoch 2/90000\tTraining Loss: 0.955\ttakes 27.000 seconds\n","Epoch 2/92000\tTraining Loss: 0.969\ttakes 28.000 seconds\n","Epoch 2/94000\tTraining Loss: 0.784\ttakes 28.000 seconds\n","Epoch 2/96000\tTraining Loss: 0.830\ttakes 28.000 seconds\n","Epoch 2/98000\tTraining Loss: 0.861\ttakes 28.000 seconds\n","Epoch 2/100000\tTraining Loss: 1.103\ttakes 28.000 seconds\n","Epoch 2/102000\tTraining Loss: 0.970\ttakes 28.000 seconds\n","Epoch 2/104000\tTraining Loss: 0.879\ttakes 28.000 seconds\n","Epoch 2/106000\tTraining Loss: 0.885\ttakes 28.000 seconds\n","Epoch 2/106756\tTraining Loss: 0.888\tDev Accuracy: 0.699\ttakes 31.000 seconds\n","Epoch 3/2000\tTraining Loss: 0.769\ttakes 31.000 seconds\n","Epoch 3/4000\tTraining Loss: 1.045\ttakes 29.000 seconds\n","Epoch 3/6000\tTraining Loss: 0.715\ttakes 27.000 seconds\n","Epoch 3/8000\tTraining Loss: 0.811\ttakes 28.000 seconds\n","Epoch 3/10000\tTraining Loss: 1.001\ttakes 28.000 seconds\n","Epoch 3/12000\tTraining Loss: 0.748\ttakes 28.000 seconds\n","Epoch 3/14000\tTraining Loss: 1.042\ttakes 28.000 seconds\n","Epoch 3/16000\tTraining Loss: 0.864\ttakes 28.000 seconds\n","Epoch 3/18000\tTraining Loss: 1.003\ttakes 28.000 seconds\n","Epoch 3/20000\tTraining Loss: 0.996\ttakes 28.000 seconds\n","Epoch 3/22000\tTraining Loss: 0.800\ttakes 28.000 seconds\n","Epoch 3/24000\tTraining Loss: 0.919\ttakes 28.000 seconds\n","Epoch 3/26000\tTraining Loss: 0.799\ttakes 28.000 seconds\n","Epoch 3/28000\tTraining Loss: 0.974\ttakes 29.000 seconds\n","Epoch 3/30000\tTraining Loss: 0.891\ttakes 28.000 seconds\n","Epoch 3/32000\tTraining Loss: 0.983\ttakes 29.000 seconds\n","Epoch 3/34000\tTraining Loss: 0.915\ttakes 28.000 seconds\n","Epoch 3/36000\tTraining Loss: 0.902\ttakes 28.000 seconds\n","Epoch 3/38000\tTraining Loss: 0.840\ttakes 29.000 seconds\n","Epoch 3/40000\tTraining Loss: 0.963\ttakes 28.000 seconds\n","Epoch 3/42000\tTraining Loss: 0.904\ttakes 29.000 seconds\n","Epoch 3/44000\tTraining Loss: 0.946\ttakes 28.000 seconds\n","Epoch 3/46000\tTraining Loss: 0.914\ttakes 27.000 seconds\n","Epoch 3/48000\tTraining Loss: 0.945\ttakes 29.000 seconds\n","Epoch 3/50000\tTraining Loss: 0.737\ttakes 29.000 seconds\n","Epoch 3/52000\tTraining Loss: 0.955\ttakes 27.000 seconds\n","Epoch 3/54000\tTraining Loss: 0.875\ttakes 28.000 seconds\n","Epoch 3/56000\tTraining Loss: 0.880\ttakes 28.000 seconds\n","Epoch 3/58000\tTraining Loss: 0.808\ttakes 28.000 seconds\n","Epoch 3/60000\tTraining Loss: 0.894\ttakes 28.000 seconds\n","Epoch 3/62000\tTraining Loss: 0.844\ttakes 28.000 seconds\n","Epoch 3/64000\tTraining Loss: 0.932\ttakes 28.000 seconds\n","Epoch 3/66000\tTraining Loss: 0.881\ttakes 28.000 seconds\n","Epoch 3/68000\tTraining Loss: 1.029\ttakes 28.000 seconds\n","Epoch 3/70000\tTraining Loss: 0.859\ttakes 28.000 seconds\n","Epoch 3/72000\tTraining Loss: 0.981\ttakes 28.000 seconds\n","Epoch 3/74000\tTraining Loss: 0.786\ttakes 28.000 seconds\n","Epoch 3/76000\tTraining Loss: 0.911\ttakes 28.000 seconds\n","Epoch 3/78000\tTraining Loss: 0.971\ttakes 28.000 seconds\n","Epoch 3/80000\tTraining Loss: 0.944\ttakes 28.000 seconds\n","Epoch 3/82000\tTraining Loss: 0.917\ttakes 28.000 seconds\n","Epoch 3/84000\tTraining Loss: 0.918\ttakes 28.000 seconds\n","Epoch 3/86000\tTraining Loss: 0.874\ttakes 28.000 seconds\n","Epoch 3/88000\tTraining Loss: 0.854\ttakes 28.000 seconds\n","Epoch 3/90000\tTraining Loss: 0.958\ttakes 29.000 seconds\n","Epoch 3/92000\tTraining Loss: 0.985\ttakes 27.000 seconds\n","Epoch 3/94000\tTraining Loss: 0.932\ttakes 28.000 seconds\n","Epoch 3/96000\tTraining Loss: 1.034\ttakes 28.000 seconds\n","Epoch 3/98000\tTraining Loss: 0.897\ttakes 27.000 seconds\n","Epoch 3/100000\tTraining Loss: 0.814\ttakes 28.000 seconds\n","Epoch 3/102000\tTraining Loss: 0.899\ttakes 28.000 seconds\n","Epoch 3/104000\tTraining Loss: 0.879\ttakes 28.000 seconds\n","Epoch 3/106000\tTraining Loss: 0.806\ttakes 28.000 seconds\n","Epoch 3/106756\tTraining Loss: 0.935\tDev Accuracy: 0.700\ttakes 31.000 seconds\n","Epoch 4/2000\tTraining Loss: 0.798\ttakes 31.000 seconds\n","Epoch 4/4000\tTraining Loss: 1.012\ttakes 29.000 seconds\n","Epoch 4/6000\tTraining Loss: 0.893\ttakes 28.000 seconds\n","Epoch 4/8000\tTraining Loss: 0.966\ttakes 28.000 seconds\n","Epoch 4/10000\tTraining Loss: 0.926\ttakes 27.000 seconds\n","Epoch 4/12000\tTraining Loss: 0.865\ttakes 27.000 seconds\n","Epoch 4/14000\tTraining Loss: 0.829\ttakes 27.000 seconds\n","Epoch 4/16000\tTraining Loss: 0.828\ttakes 27.000 seconds\n","Epoch 4/18000\tTraining Loss: 0.805\ttakes 28.000 seconds\n","Epoch 4/20000\tTraining Loss: 0.899\ttakes 28.000 seconds\n","Epoch 4/22000\tTraining Loss: 0.959\ttakes 27.000 seconds\n","Epoch 4/24000\tTraining Loss: 0.935\ttakes 28.000 seconds\n","Epoch 4/26000\tTraining Loss: 0.856\ttakes 28.000 seconds\n","Epoch 4/28000\tTraining Loss: 0.854\ttakes 28.000 seconds\n","Epoch 4/30000\tTraining Loss: 0.837\ttakes 27.000 seconds\n","Epoch 4/32000\tTraining Loss: 0.850\ttakes 27.000 seconds\n","Epoch 4/34000\tTraining Loss: 0.818\ttakes 28.000 seconds\n","Epoch 4/36000\tTraining Loss: 0.884\ttakes 28.000 seconds\n","Epoch 4/38000\tTraining Loss: 0.992\ttakes 28.000 seconds\n","Epoch 4/40000\tTraining Loss: 1.030\ttakes 29.000 seconds\n","Epoch 4/42000\tTraining Loss: 0.691\ttakes 28.000 seconds\n","Epoch 4/44000\tTraining Loss: 1.088\ttakes 29.000 seconds\n","Epoch 4/46000\tTraining Loss: 0.888\ttakes 28.000 seconds\n","Epoch 4/48000\tTraining Loss: 0.867\ttakes 29.000 seconds\n","Epoch 4/50000\tTraining Loss: 0.775\ttakes 28.000 seconds\n","Epoch 4/52000\tTraining Loss: 0.788\ttakes 28.000 seconds\n","Epoch 4/54000\tTraining Loss: 0.921\ttakes 28.000 seconds\n","Epoch 4/56000\tTraining Loss: 0.906\ttakes 27.000 seconds\n","Epoch 4/58000\tTraining Loss: 0.844\ttakes 28.000 seconds\n","Epoch 4/60000\tTraining Loss: 0.834\ttakes 28.000 seconds\n","Epoch 4/62000\tTraining Loss: 0.781\ttakes 28.000 seconds\n","Epoch 4/64000\tTraining Loss: 0.800\ttakes 28.000 seconds\n","Epoch 4/66000\tTraining Loss: 0.751\ttakes 28.000 seconds\n","Epoch 4/68000\tTraining Loss: 0.792\ttakes 29.000 seconds\n","Epoch 4/70000\tTraining Loss: 0.912\ttakes 28.000 seconds\n","Epoch 4/72000\tTraining Loss: 0.790\ttakes 28.000 seconds\n","Epoch 4/74000\tTraining Loss: 0.819\ttakes 28.000 seconds\n","Epoch 4/76000\tTraining Loss: 0.889\ttakes 27.000 seconds\n","Epoch 4/78000\tTraining Loss: 0.780\ttakes 27.000 seconds\n","Epoch 4/80000\tTraining Loss: 0.760\ttakes 28.000 seconds\n","Epoch 4/82000\tTraining Loss: 0.950\ttakes 27.000 seconds\n","Epoch 4/84000\tTraining Loss: 0.929\ttakes 28.000 seconds\n","Epoch 4/86000\tTraining Loss: 0.815\ttakes 28.000 seconds\n","Epoch 4/88000\tTraining Loss: 0.994\ttakes 28.000 seconds\n","Epoch 4/90000\tTraining Loss: 0.995\ttakes 27.000 seconds\n","Epoch 4/92000\tTraining Loss: 1.041\ttakes 28.000 seconds\n","Epoch 4/94000\tTraining Loss: 0.973\ttakes 27.000 seconds\n","Epoch 4/96000\tTraining Loss: 0.887\ttakes 28.000 seconds\n","Epoch 4/98000\tTraining Loss: 0.864\ttakes 28.000 seconds\n","Epoch 4/100000\tTraining Loss: 0.941\ttakes 28.000 seconds\n","Epoch 4/102000\tTraining Loss: 0.964\ttakes 27.000 seconds\n","Epoch 4/104000\tTraining Loss: 0.839\ttakes 28.000 seconds\n","Epoch 4/106000\tTraining Loss: 0.851\ttakes 28.000 seconds\n","Epoch 4/106756\tTraining Loss: 0.931\tDev Accuracy: 0.701\ttakes 30.000 seconds\n","Finished Training\n"]}],"source":["import time\n","\n","before = time.time()\n","\n","model_prefix = \"model_\"\n","scheduler.step()\n","for epoch in range(NUM_EPOCHS):\n","    model.train()\n","    \n","    train_for_one_epoch(model = model, train_loader = train_loader, dev_loader = dev_loader,  \n","                        optimizer = optimizer, model_name = model_prefix + str(epoch), epoch = epoch,\n","                        scheduler = scheduler, if_save = True)\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JhQP2N05cKDN"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":307},"executionInfo":{"elapsed":98688,"status":"error","timestamp":1608395263595,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"hZmqR32gxMQG","outputId":"fc6688f1-7703-4c89-b344-1b46b16340de"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 5/2000\tTraining Loss: 1.006\ttakes 32.000 seconds\n","Epoch 5/4000\tTraining Loss: 0.917\ttakes 29.000 seconds\n","Epoch 5/6000\tTraining Loss: 0.987\ttakes 29.000 seconds\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-52-4d563bbfc0b4\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     train_for_one_epoch(model = model, train_loader = train_loader, dev_loader = dev_loader,  \n\u001b[1;32m     12\u001b[0m                         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 13\u001b[0;31m                         scheduler = scheduler, if_save = True)\n\u001b[0m","\u001b[0;32m\u003cipython-input-45-da4fed73910e\u003e\u001b[0m in \u001b[0;36mtrain_for_one_epoch\u001b[0;34m(model, train_loader, dev_loader, optimizer, model_name, epoch, scheduler, if_save)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 23\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update the weights using the computed gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 84\u001b[0;31m                     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                     \u001b[0;31m# Lazy state initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0mrelevant_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 598\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__hash__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["optimizer = optim.Adam(model.parameters(), lr = 1e-4, weight_decay = 5e-5)\n","lmbda = lambda epoch: 0.85\n","scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)\n","\n","\n","import time\n","\n","before = time.time()\n","\n","for epoch in range(8, 8 + NUM_EPOCHS):\n","    train_for_one_epoch(model = model, train_loader = train_loader, dev_loader = dev_loader,  \n","                        optimizer = optimizer, model_name = model_prefix + str(epoch), epoch = epoch,\n","                        scheduler = scheduler, if_save = True)"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2122,"status":"ok","timestamp":1608395115278,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"lfnxRFUY0Z5w","outputId":"b70f4eff-3f8d-454c-b5cf-b6c45e2d40c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Help on class CosineAnnealingLR in module torch.optim.lr_scheduler:\n","\n","class CosineAnnealingLR(_LRScheduler)\n"," |  Set the learning rate of each parameter group using a cosine annealing\n"," |  schedule, where :math:`\\eta_{max}` is set to the initial lr and\n"," |  :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n"," |  \n"," |  .. math::\n"," |      \\begin{aligned}\n"," |          \\eta_t \u0026 = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1\n"," |          + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right),\n"," |          \u0026 T_{cur} \\neq (2k+1)T_{max}; \\\\\n"," |          \\eta_{t+1} \u0026 = \\eta_{t} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\n"," |          \\left(1 - \\cos\\left(\\frac{1}{T_{max}}\\pi\\right)\\right),\n"," |          \u0026 T_{cur} = (2k+1)T_{max}.\n"," |      \\end{aligned}\n"," |  \n"," |  When last_epoch=-1, sets initial lr as lr. Notice that because the schedule\n"," |  is defined recursively, the learning rate can be simultaneously modified\n"," |  outside this scheduler by other operators. If the learning rate is set\n"," |  solely by this scheduler, the learning rate at each step becomes:\n"," |  \n"," |  .. math::\n"," |      \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 +\n"," |      \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right)\n"," |  \n"," |  It has been proposed in\n"," |  `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only\n"," |  implements the cosine annealing part of SGDR, and not the restarts.\n"," |  \n"," |  Args:\n"," |      optimizer (Optimizer): Wrapped optimizer.\n"," |      T_max (int): Maximum number of iterations.\n"," |      eta_min (float): Minimum learning rate. Default: 0.\n"," |      last_epoch (int): The index of last epoch. Default: -1.\n"," |      verbose (bool): If ``True``, prints a message to stdout for\n"," |          each update. Default: ``False``.\n"," |  \n"," |  .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n"," |      https://arxiv.org/abs/1608.03983\n"," |  \n"," |  Method resolution order:\n"," |      CosineAnnealingLR\n"," |      _LRScheduler\n"," |      builtins.object\n"," |  \n"," |  Methods defined here:\n"," |  \n"," |  __init__(self, optimizer, T_max, eta_min=0, last_epoch=-1, verbose=False)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |  \n"," |  get_lr(self)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from _LRScheduler:\n"," |  \n"," |  get_last_lr(self)\n"," |      Return last computed learning rate by current scheduler.\n"," |  \n"," |  load_state_dict(self, state_dict)\n"," |      Loads the schedulers state.\n"," |      \n"," |      Arguments:\n"," |          state_dict (dict): scheduler state. Should be an object returned\n"," |              from a call to :meth:`state_dict`.\n"," |  \n"," |  print_lr(self, is_verbose, group, lr, epoch=None)\n"," |      Display the current learning rate.\n"," |  \n"," |  state_dict(self)\n"," |      Returns the state of the scheduler as a :class:`dict`.\n"," |      \n"," |      It contains an entry for every variable in self.__dict__ which\n"," |      is not the optimizer.\n"," |  \n"," |  step(self, epoch=None)\n"," |  \n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from _LRScheduler:\n"," |  \n"," |  __dict__\n"," |      dictionary for instance variables (if defined)\n"," |  \n"," |  __weakref__\n"," |      list of weak references to the object (if defined)\n","\n"]}],"source":["help(CosineAnnealingLR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhnZzXVB0Pf2"},"outputs":[],"source":["optimizer = optim.Adam(model.parameters(), lr = 1e-4, weight_decay = 5e-5)\n","lmbda = lambda epoch: 0.85\n","scheduler = CosineAnnealingLR(optimizer, eta_min=1e-6, verbose=True)\n","\n","\n","import time\n","\n","before = time.time()\n","\n","for epoch in range(4, 4 + NUM_EPOCHS):\n","    train_for_one_epoch(model = model, train_loader = train_loader, dev_loader = dev_loader,  \n","                        optimizer = optimizer, model_name = model_prefix + str(epoch), epoch = epoch,\n","                        scheduler = scheduler, if_save = True)"]},{"cell_type":"markdown","metadata":{"id":"dqGlrH3sZAlQ"},"source":["## 5 generate submission file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_2G1DwDApxS"},"outputs":[],"source":["def load_model(model_file, model, optimizer, scheduler):\n","    temp = torch.load(\"/content/gdrive/My Drive/11685deeplearning/\" + model_file) \n","    model.load_state_dict(temp['model_state_dict'])\n","    optimizer.load_state_dict(temp['optimizer_state_dict'])\n","    scheduler.load_state_dict(temp['scheduler_state_dict'])\n","\n","    return model, optimizer, scheduler\n","\n","\n","def submit(model, optimizer, scheduler, model_name, test_file = \"test.npy\"):\n","    # load model\n","  # model_name = \"Model_1_7\"\n","\n","  # model, optimizer, scheduler = \n","  temp = torch.load(\"/content/gdrive/My Drive/11685deeplearning/\" + model_name) \n","  model.load_state_dict(temp['model_state_dict'])\n","  optimizer.load_state_dict(temp['optimizer_state_dict'])\n","  scheduler.load_state_dict(temp['scheduler_state_dict'])\n","\n","  test = np.load(DATA_PATH + test_file, allow_pickle=True)\n","  test_labels = np.zeros((np.concatenate(test).shape[0], 1))\n","\n","  test_dataset = hw1Dataset(test, test_labels, context_size)\n","  test_loader_args = dict(shuffle = False, batch_size = 1024,\n","                          num_workers = num_workers, pin_memory = True)\\\n","  if cuda else dict(shuffle = False, batch_szie = 64)\n","\n","  test_loader = DataLoader(test_dataset, **test_loader_args)\n","\n","\n","  model.eval()\n","  predicted_list = []\n","\n","  with torch.no_grad():\n","    for idx, (data, labels) in enumerate(test_loader):\n","      data = data.to(device)\n","      labels = labels.to(device)\n","            \n","      out = model.forward(data)\n","      _, predicted = torch.max(out.data, 1)\n","\n","      predicted_list.append(predicted.cpu().data.numpy())\n","\n","  import pandas as pd\n","  model.train()\n","\n","  submit = pd.DataFrame(enumerate(np.concatenate(predicted_list)), columns = ['id', 'label'])\n","  submit.to_csv(model_name + \"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":24417,"status":"ok","timestamp":1601882494962,"user":{"displayName":"Kexin Zhang","photoUrl":"","userId":"03320055183831526059"},"user_tz":-480},"id":"ffIhmfK5BH5b","outputId":"b9c6ff93-8cb7-4cab-8c1b-77422461edc1"},"outputs":[{"name":"stdout","output_type":"stream","text":["# of examples: 1593253 // 1593223\n"]}],"source":["model_name = \"Model_3_6\"\n","submit(model, optimizer, scheduler, model_name)"]},{"cell_type":"markdown","metadata":{"id":"ssWuLmHqq4OX"},"source":["# New Section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M89MJQbGomwf"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"2ftxw3Xbolvc"},"source":["# New Section"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lL9gs0WG9QDO"},"outputs":[],"source":["## 6 Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8gB9gNNskVXY"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"makeup-hw1p2-kexinzha-2.ipynb","provenance":[{"file_id":"1HvNf57xLcUkybl7xtVcc_Fv4Q3cINjMP","timestamp":1601883049483},{"file_id":"1DAk2_AsPU4VYdcz31FF22bzA_dF88hPi","timestamp":1601865691872},{"file_id":"1von01bIS8ng357yJU8VCJRSbLoDPw3rW","timestamp":1601863237611}],"toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}